{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Ahmed Baari. 126156004. 6th G.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA on Text Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "story_text = \"\"\"3 years ago, Baari joined SASTRA University. The initial days were exciting, with many new cool places to explore, hangout and have fun. Most teachers were chill too. Eventually, the horror of the academics had shown itself during the end semester exams. Winter vacation was only a week long, and second semester had a really bad start. Baari was struggling with both academics and maintaing other things, and that eventually caused his CGPA to drop even more by the end of that semester. Everything changed when the 3rd year began.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving it as a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file created successfully!\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "# Split the text into sentences\n",
    "sentences = story_text.split(\". \")\n",
    "\n",
    "# Write the sentences to a CSV file\n",
    "with open(\"story.csv\", \"w\", newline=\"\") as csvfile:\n",
    "  writer = csv.writer(csvfile)\n",
    "  for sentence in sentences:\n",
    "    writer.writerow([sentence])\n",
    "\n",
    "print(\"CSV file created successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading from CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def read_csv_to_string(filename):\n",
    "  content = \"\"\n",
    "  with open(filename, \"r\") as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    \n",
    "    for row in reader:\n",
    "      row_string = \" \".join(row)  # space delimiter\n",
    "      content += row_string + \". \"  # Next sentence\n",
    "  return content.strip()  # Remove leading/trailing whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2 years ago, Baari joined SASTRA University. The initial days were exciting, with many new cool places to explore, hangout and have fun. Most teachers were chill too. Eventually, the horror of the academics had shown itself during the end semester exams. Winter vacation was only a week long, and second semester had a really bad start. Baari was struggling with both academics and maintaing other things, and that eventually caused his CGPA to drop even more by the end of that semester. Everything changed when the 3rd year began..'"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = \"story.csv\"  # Replace with your CSV filename\n",
    "my_text = read_csv_to_string(filename)\n",
    "\n",
    "my_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Breaking up a piece of text into smaller parts, such as sentences and words.  \n",
    "These pieces are called tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`word_tokenize` module, used above is basically a wrapper function that calls tokenize() function as an instance of the `TreebankWordTokenizer` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['to', 'explore', ',', 'hangout', 'and']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TreeBankTokenized = word_tokenize(my_text)\n",
    "TreeBankTokenized[20:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, `WordPunctTokenizer` makes sure to split all punctuation into separate tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "PunktTokenizer = WordPunctTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2',\n",
       " 'years',\n",
       " 'ago',\n",
       " ',',\n",
       " 'Baari',\n",
       " 'joined',\n",
       " 'SASTRA',\n",
       " 'University',\n",
       " '.',\n",
       " 'The',\n",
       " 'initial',\n",
       " 'days',\n",
       " 'were',\n",
       " 'exciting',\n",
       " ',',\n",
       " 'with',\n",
       " 'many',\n",
       " 'new',\n",
       " 'cool',\n",
       " 'places']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PunktTokenized =  PunktTokenizer.tokenize(my_text)\n",
    "PunktTokenized[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can tokenize into **sentences** or using **regular expressions** too, but here I'll focus on words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Stop Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Words like `by`, `on`, `the` and `a` are stop words. They do not contribute in the meaning of a sentence.  \n",
    "NLTK's stopword corpus contains the word list of stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the list of english stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['your', 'over', 'yours', 'just', 'do', 'by', 'no', 'they', 'further', 'd']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_stops = set(stopwords.words('english'))\n",
    "\n",
    "# Displaying some of the english stop words\n",
    "list(english_stops)[:10] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's remove the stop words from our text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['initial',\n",
       " 'days',\n",
       " 'were',\n",
       " 'exciting',\n",
       " ',',\n",
       " 'with',\n",
       " 'many',\n",
       " 'new',\n",
       " 'cool',\n",
       " 'places']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PunktTokenized[10:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['initial',\n",
       " 'days',\n",
       " 'exciting',\n",
       " ',',\n",
       " 'many',\n",
       " 'new',\n",
       " 'cool',\n",
       " 'places',\n",
       " 'explore',\n",
       " ',']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words_removed = [word for word in PunktTokenized if word not in english_stops]\n",
    "stop_words_removed[10:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the difference between the outputs of the above 2 cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*wordnet skipped*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming is a technique used to **extract the base form of the words**.  \n",
    "This is done by removing affixes from them.  \n",
    "Eg: the stem of the words `eating`, `eats`, `eaten` is **`eat`**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most popular stemming algorithm is the `PorterStemmer` algorithm. Let's use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "word_stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'run'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_stemmer.stem('running')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But also, stemming has its drawbacks. Below example shows how `happily` is stemmed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'happili'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_stemmer.stem('happily')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how our text is after stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2', 'year', 'ago', ',', 'baari', 'join', 'sastra', 'univers', '.', 'the']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmed_words = []\n",
    "for word in stop_words_removed:\n",
    "  stemmed_words.append(word_stemmer.stem(word))\n",
    "\n",
    "stemmed_words[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatization technique is like stemming.  \n",
    "It converts all the words to their root words or `lemma`.  \n",
    "These words are valid words unlike the output by stemming.\n",
    "\n",
    "We can use `WordNetLemmatizer` for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'academic'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('academics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2',\n",
       " 'year',\n",
       " 'ago',\n",
       " ',',\n",
       " 'Baari',\n",
       " 'joined',\n",
       " 'SASTRA',\n",
       " 'University',\n",
       " '.',\n",
       " 'The']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatized_words = []\n",
    "for word in stop_words_removed:\n",
    "  lemmatized_words.append(lemmatizer.lemmatize(word))\n",
    "\n",
    "lemmatized_words[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synonym Replacement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replacing a word with it's synonym.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordSynReplacer:\n",
    "  def __init__(self, word_map):\n",
    "    self.word_map = word_map\n",
    "\n",
    "  def replace(self, words):\n",
    "    return [self.word_map.get(word, word) for word in words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For my usecase, I am replacing all instances of `Baari` with `student`. This demonstrates **identifying named entities** and replacing them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2',\n",
       " 'year',\n",
       " 'ago',\n",
       " ',',\n",
       " 'student',\n",
       " 'joined',\n",
       " 'SASTRA',\n",
       " 'University',\n",
       " '.',\n",
       " 'The']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rep_syn = WordSynReplacer({'Baari': 'student'})\n",
    "syn_replaced = rep_syn.replace(lemmatized_words)\n",
    "syn_replaced[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Antonym replacement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uploaded to the class group in [this gist](https://gist.github.com/AhmedBaari/1302fc76eb0751e3bc137776b74e75e1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "class word_antonym_replacer(object):\n",
    "    def replace(self, word, pos=None):\n",
    "        antonyms = set()\n",
    "        for syn in wordnet.synsets(word, pos=pos):\n",
    "            for lemma in syn.lemmas():\n",
    "                for antonym in lemma.antonyms():\n",
    "                    antonyms.add(antonym.name())\n",
    "        if len(antonyms) == 1:\n",
    "            return antonyms.pop()\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    def replace_negations(self, sent):\n",
    "        i, l = 0, len(sent)\n",
    "        words = []\n",
    "        while i < l:\n",
    "            word = sent[i]\n",
    "            if word == 'not' and i+1 < l:\n",
    "                ant = self.replace(sent[i+1])\n",
    "                if ant:\n",
    "                    words.append(ant)\n",
    "                    i += 2\n",
    "                    continue\n",
    "            words.append(word)\n",
    "            i += 1\n",
    "        return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'heat'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "rep_antonym = word_antonym_replacer()\n",
    "rep_antonym.replace(\"chill\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of replacing the negations of a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Let us', 'beautify', 'our', 'country']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = [\"Let us\", 'not', 'uglify', 'our', 'country']\n",
    "rep_antonym.replace_negations(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's use this in our text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['initial', 'day', 'exciting', ',', 'many']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentence = [\"Let us\", 'not', 'uglify', 'our', 'country']\n",
    "antomym_replaced = rep_antonym.replace_negations(syn_replaced)\n",
    "antomym_replaced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "difference = set(syn_replaced) ^ set(antomym_replaced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nothing was changed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tagging is the automatic assignment of the description of the tokens into one of the parts of speech  \n",
    "- nouns\n",
    "- verb\n",
    "- adverbs\n",
    "- adjectives\n",
    "- pronouns\n",
    "- conjunction  \n",
    "and their sub-categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'PRP'),\n",
       " ('will', 'MD'),\n",
       " ('be', 'VB'),\n",
       " ('studying', 'VBG'),\n",
       " ('at', 'IN'),\n",
       " ('SASTRA', 'NNP'),\n",
       " ('Deemed', 'NNP'),\n",
       " ('University', 'NNP')]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"I will be studying at SASTRA Deemed University\"\n",
    "nltk.pos_tag(word_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Abbreviation | Part of Speech | Description |\n",
    "|---|---|---|\n",
    "| PRP | Personal pronoun | Pronouns referring to specific people or things (e.g., I, you, he, she, it, we, they) |\n",
    "| VB | Verb, base form | The infinitive form of a verb (e.g., run, jump, eat) |\n",
    "| VBG | Verb, gerund/present participle | Verb form ending in \"-ing\" used as a noun (gerund) or an adjective (present participle) (e.g., running, singing, talking) |\n",
    "| NNP | Proper noun, singular | Names of specific entities (e.g., Alice, India, Google) |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('maintaing', 'VBG'),\n",
       " ('thing', 'NN'),\n",
       " (',', ','),\n",
       " ('eventually', 'RB'),\n",
       " ('caused', 'VBD'),\n",
       " ('CGPA', 'NNP'),\n",
       " ('drop', 'NN'),\n",
       " ('even', 'RB'),\n",
       " ('end', 'VBP'),\n",
       " ('semester', 'NN')]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "POS_Tagged = nltk.pos_tag(antomym_replaced)\n",
    "POS_Tagged[50:60]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yet to be covered in the syllabus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S (NP (A The) (NN sun)) (VP (V moves) (NN eastward)))\n",
      "\\Tree [.S\n",
      "        [.NP [.A The ] [.NN sun ] ]\n",
      "        [.VP [.V moves ] [.NN eastward ] ] ]\n",
      "             S                    \n",
      "      _______|_________            \n",
      "     NP                VP         \n",
      "  ___|___          ____|_____      \n",
      " A       NN       V          NN   \n",
      " |       |        |          |     \n",
      "The     sun     moves     eastward\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.tree import *\n",
    "\n",
    "left = Tree('NP', [Tree('A', ['The']), Tree('NN', ['sun'])])\n",
    "\n",
    "right = Tree('VP', [Tree('V', ['moves']), Tree('NN',\n",
    "['eastward'])])\n",
    "\n",
    "tree = Tree('S', [left, right])\n",
    "\n",
    "print(tree)\n",
    "print(tree.pformat_latex_qtree())\n",
    "tree.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In NLP, it is a standard practice to remove punctuation too. A simple function can do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(words):\n",
    "  table = str.maketrans('', '', punctuation)\n",
    "  filtered_words = [word.translate(table) for word in words]\n",
    "  return [word for word in filtered_words if word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2', 'year', 'ago', 'student', 'joined', 'SASTRA', 'University', 'The', 'initial', 'day', 'exciting', 'many', 'new', 'cool', 'place', 'explore', 'hangout', 'fun', 'Most', 'teacher', 'chill', 'Eventually', 'horror', 'academic', 'shown', 'end', 'semester', 'exam', 'Winter', 'vacation', 'week', 'long', 'second', 'semester', 'really', 'bad', 'start', 'student', 'struggling', 'academic', 'maintaing', 'thing', 'eventually', 'caused', 'CGPA', 'drop', 'even', 'end', 'semester', 'Everything', 'changed', '3rd', 'year', 'began']\n"
     ]
    }
   ],
   "source": [
    "cleaned_words = remove_punctuation(antomym_replaced)\n",
    "print(cleaned_words)  # Output: ['Hello', 'world', 'This', 'is', 'an', 'example']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now applied the popular NLP techniques.  \n",
    "Now, let us see how the final output is different from the initial text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2 years ago, Baari joined SASTRA University. The initial days were exciting, with many new cool places to explore, hangout and have fun. Most teachers were chill too. Eventually, the horror of the academics had shown itself during the end semester exams. Winter vacation was only a week long, and second semester had a really bad start. Baari was struggling with both academics and maintaing other things, and that eventually caused his CGPA to drop even more by the end of that semester. Everything changed when the 3rd year began.'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2 year ago student joined SASTRA University The initial day exciting many new cool place explore hangout fun Most teacher chill Eventually horror academic shown end semester exam Winter vacation week long second semester really bad start student struggling academic maintaing thing eventually caused CGPA drop even end semester Everything changed 3rd year began '"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_sentence = \"\"\n",
    "for i in cleaned_words:\n",
    "    final_sentence += i + \" \"\n",
    "final_sentence # Final Output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
